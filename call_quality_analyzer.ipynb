{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyMxIVgK0iDlUfemtrasd8Nw",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/saketpandey0/call-quality-analyzer/blob/main/call_quality_analyzer.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pyannote.audio librosa transformers torch --quiet\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A50FKWH8Pk3d",
        "outputId": "b3ef3d32-35e5-4832-e4ed-28471e66fe20"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m59.6/59.6 kB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m897.8/897.8 kB\u001b[0m \u001b[31m30.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m828.5/828.5 kB\u001b[0m \u001b[31m57.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.5/58.5 kB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m48.1/48.1 kB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m51.4/51.4 kB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.8/127.8 kB\u001b[0m \u001b[31m12.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m864.1/864.1 kB\u001b[0m \u001b[31m61.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m87.2/87.2 kB\u001b[0m \u001b[31m9.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m48.5/48.5 kB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m983.2/983.2 kB\u001b[0m \u001b[31m64.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m400.9/400.9 kB\u001b[0m \u001b[31m34.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m832.4/832.4 kB\u001b[0m \u001b[31m56.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m119.7/119.7 kB\u001b[0m \u001b[31m12.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m754.1/754.1 kB\u001b[0m \u001b[31m52.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for docopt (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for julius (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "qa67fzUOCCUu"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import sys\n",
        "import json\n",
        "import logging\n",
        "from pathlib import Path\n",
        "from datetime import datetime\n",
        "from typing import Dict, List, Optional, Tuple\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import librosa\n",
        "import soundfile as sf\n",
        "from pydub import AudioSegment\n",
        "\n",
        "# from transformers import Wav2Vec2ForCTC, Wav2Vec2Tokenizer\n",
        "# from pyannote.audio import Pipeline"
      ],
      "metadata": {
        "id": "HbtN_pxAIXqs"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "try:\n",
        "  from transformers import pipeline\n",
        "  TRANSFORMERS_AVAILABLE = True\n",
        "except ImportError as e:\n",
        "  print(f\"Warning: transformer not available: {e}\")\n",
        "  TRANSFORMERS_AVAILABLE = False\n"
      ],
      "metadata": {
        "id": "a4Fc1jzrQ9V_"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "WHISPER_MODEL_SIZE = \"tiny.en\"\n",
        "SAMPLE_RATE = 16000\n",
        "MAX_AUDIO_LENGTH = 300"
      ],
      "metadata": {
        "id": "NUDx5PnPUSpo"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class OptimisedCallQualityAnalyzer:\n",
        "\n",
        "  def __init__(self, whisper_model_size: str = \"tiny.en\", enable_streo_seperation: bool = True, log_level: str = \"INFO\"):\n",
        "\n",
        "    logging.basicConfig(level=getattr(logging, log_level.upper()))\n",
        "    self.logger = logging.getLogger(__name__)\n",
        "\n",
        "    self.enable_streo_seperation = enable_streo_seperation\n",
        "    self.whisper_model_size = whisper_model_size\n",
        "\n",
        "    self.whisper_model = None\n",
        "    self.sentiment_analyzer = None\n",
        "\n",
        "    self._initialize_models()\n",
        "    self.logger.info(\"OptimizedCallQualityAnalyzer initialized successfully\")\n"
      ],
      "metadata": {
        "id": "sx9iYHHvUuaa"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def _initialize_models(self):\n",
        "        \"\"\"Initialize lightweight ML models for speed\"\"\"\n",
        "\n",
        "        try:\n",
        "            # Load fastest Whisper model\n",
        "            self.logger.info(f\"Loading Whisper model ({self.whisper_model_size})...\")\n",
        "            start_time = datetime.now()\n",
        "            self.whisper_model = whisper.load_model(self.whisper_model_size)\n",
        "            load_time = (datetime.now() - start_time).total_seconds()\n",
        "            self.logger.info(f\"Whisper loaded in {load_time:.1f}s\")\n",
        "\n",
        "            if TRANSFORMERS_AVAILABLE:\n",
        "                self.logger.info(\"Loading lightweight sentiment model...\")\n",
        "                try:\n",
        "                    self.sentiment_analyzer = pipeline(\n",
        "                        \"sentiment-analysis\",\n",
        "                        model=\"distilbert-base-uncased-finetuned-sst-2-english\",\n",
        "                        return_all_scores=True\n",
        "                    )\n",
        "                    self.logger.info(\"Sentiment analyzer loaded\")\n",
        "                except Exception as e:\n",
        "                    self.logger.warning(f\"Failed to load sentiment model: {e}\")\n",
        "                    try:\n",
        "                        self.sentiment_analyzer = pipeline(\"sentiment-analysis\")\n",
        "                    except Exception as e2:\n",
        "                        self.logger.error(f\"No sentiment model available: {e2}\")\n",
        "                        self.sentiment_analyzer = None\n",
        "            else:\n",
        "                self.logger.warning(\"Transformers not available - sentiment analysis disabled\")\n",
        "                self.sentiment_analyzer = None\n",
        "\n",
        "        except Exception as e:\n",
        "            self.logger.error(f\"Error initializing models: {e}\")\n",
        "            raise\n",
        ""
      ],
      "metadata": {
        "id": "e02d-gLxWh8R"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_audio_fast(self,\n",
        "                             audio_path: str,\n",
        "                             target_sr: int = 16000,\n",
        "                             max_length: int = MAX_AUDIO_LENGTH) -> Tuple[Optional[np.ndarray], Optional[int], Optional[float], Optional[bool]]:\n",
        "\n",
        "\n",
        "        self.logger.info(f\"Fast preprocessing: {Path(audio_path).name}\")\n",
        "\n",
        "        try:\n",
        "            # Load audio - limit length for speed\n",
        "            audio, sr = librosa.load(audio_path, sr=target_sr, duration=max_length)\n",
        "\n",
        "            # Check if original was stereo\n",
        "            try:\n",
        "                # Try to load original to check channels\n",
        "                original_audio, original_sr = librosa.load(audio_path, sr=None, mono=False)\n",
        "                is_stereo = len(original_audio.shape) > 1 and original_audio.shape[0] == 2\n",
        "            except:\n",
        "                is_stereo = False\n",
        "\n",
        "            # Simple normalization\n",
        "            if np.max(np.abs(audio)) > 0:\n",
        "                audio = audio / np.max(np.abs(audio)) * 0.95\n",
        "\n",
        "            duration = len(audio) / sr\n",
        "            self.logger.info(f\"Audio processed: {duration:.2f}s at {sr}Hz, Stereo: {is_stereo}\")\n",
        "\n",
        "            return audio, sr, duration, is_stereo\n",
        "\n",
        "        except Exception as e:\n",
        "            self.logger.error(f\"Error preprocessing audio: {e}\")\n",
        "            return None, None, None, None"
      ],
      "metadata": {
        "id": "FBmBHE-R47s8"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def separate_stereo_channels(self, audio_path: str) -> Dict[str, np.ndarray]:\n",
        "\n",
        "\n",
        "        self.logger.info(\"Separating stereo channels...\")\n",
        "\n",
        "        try:\n",
        "            # Load as stereo\n",
        "            audio, sr = librosa.load(audio_path, sr=SAMPLE_RATE, mono=False)\n",
        "\n",
        "            if len(audio.shape) == 1:\n",
        "                # Mono audio - can't separate\n",
        "                self.logger.warning(\"Mono audio detected - cannot separate channels\")\n",
        "                return {\n",
        "                    'agent': audio,\n",
        "                    'customer': audio,\n",
        "                    'separation_method': 'mono_duplicate'\n",
        "                }\n",
        "\n",
        "            elif audio.shape[0] == 2:\n",
        "                # True stereo\n",
        "                left_channel = audio[0]\n",
        "                right_channel = audio[1]\n",
        "\n",
        "                # Assume agent on left, customer on right\n",
        "                return {\n",
        "                    'agent': left_channel,\n",
        "                    'customer': right_channel,\n",
        "                    'separation_method': 'stereo_channels'\n",
        "                }\n",
        "\n",
        "            else:\n",
        "                # Multi-channel - use first two\n",
        "                self.logger.warning(f\"Multi-channel audio ({audio.shape[0]} channels) - using first two\")\n",
        "                return {\n",
        "                    'agent': audio[0],\n",
        "                    'customer': audio[1],\n",
        "                    'separation_method': 'multi_channel'\n",
        "                }\n",
        "\n",
        "        except Exception as e:\n",
        "            self.logger.error(f\"Error in stereo separation: {e}\")\n",
        "            # Fallback to mono\n",
        "            try:\n",
        "                audio, sr = librosa.load(audio_path, sr=SAMPLE_RATE, mono=True)\n",
        "                return {\n",
        "                    'agent': audio,\n",
        "                    'customer': audio,\n",
        "                    'separation_method': 'fallback_mono'\n",
        "                }\n",
        "            except:\n",
        "                return {}\n",
        ""
      ],
      "metadata": {
        "id": "ws0DP4ty5XdW"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def transcribe_audio_fast(self, audio_path: str) -> Optional[Dict]:\n",
        "\n",
        "\n",
        "        self.logger.info(\"Fast transcription...\")\n",
        "        start_time = datetime.now()\n",
        "\n",
        "        try:\n",
        "            # Use fastest settings\n",
        "            result = self.whisper_model.transcribe(\n",
        "                audio_path,\n",
        "                language=\"en\",  # Force English for speed\n",
        "                word_timestamps=False,  # Disable for speed\n",
        "                verbose=False\n",
        "            )\n",
        "\n",
        "            transcription_time = (datetime.now() - start_time).total_seconds()\n",
        "            text_length = len(result['text'])\n",
        "\n",
        "            self.logger.info(f\"Transcription completed in {transcription_time:.1f}s: {text_length} chars\")\n",
        "\n",
        "            return result\n",
        "\n",
        "        except Exception as e:\n",
        "            self.logger.error(f\"Error in fast transcription: {e}\")\n",
        "            return None"
      ],
      "metadata": {
        "id": "K4mD9V9D51dq"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_talk_time_from_channels(self, channel_data: Dict[str, np.ndarray], sample_rate: int) -> Dict[str, float]:\n",
        "\n",
        "\n",
        "        if not channel_data or len(channel_data) < 2:\n",
        "            return {\"Unknown Speaker\": 100.0}\n",
        "\n",
        "        try:\n",
        "            # Calculate RMS energy for each channel\n",
        "            window_size = int(0.1 * sample_rate)  # 100ms windows\n",
        "            energy_threshold = 0.01  # Minimum energy to consider as speech\n",
        "\n",
        "            channel_speaking_time = {}\n",
        "\n",
        "            for speaker, audio in channel_data.items():\n",
        "                if speaker in ['separation_method']:\n",
        "                    continue\n",
        "\n",
        "                # Calculate windowed RMS energy\n",
        "                speaking_time = 0\n",
        "                for i in range(0, len(audio) - window_size, window_size):\n",
        "                    window = audio[i:i + window_size]\n",
        "                    rms = np.sqrt(np.mean(window**2))\n",
        "\n",
        "                    if rms > energy_threshold:\n",
        "                        speaking_time += window_size / sample_rate\n",
        "\n",
        "                channel_speaking_time[speaker] = speaking_time\n",
        "\n",
        "            # Convert to percentages\n",
        "            total_speaking_time = sum(channel_speaking_time.values())\n",
        "\n",
        "            if total_speaking_time == 0:\n",
        "                return {\"agent\": 50.0, \"customer\": 50.0}\n",
        "\n",
        "            talk_ratios = {}\n",
        "            for speaker, time in channel_speaking_time.items():\n",
        "                percentage = (time / total_speaking_time) * 100\n",
        "                talk_ratios[speaker] = round(percentage, 2)\n",
        "\n",
        "            return talk_ratios\n",
        "\n",
        "        except Exception as e:\n",
        "            self.logger.error(f\"Error calculating talk ratios: {e}\")\n",
        "            return {\"agent\": 50.0, \"customer\": 50.0}\n",
        ""
      ],
      "metadata": {
        "id": "Jbw2Hxwx6FRg"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def count_questions_fast(self, text: str) -> Dict:\n",
        "\n",
        "\n",
        "        import re\n",
        "\n",
        "        # Simple question patterns for speed\n",
        "        questions = []\n",
        "\n",
        "        # Split into sentences quickly\n",
        "        sentences = re.split(r'[.!?]+', text)\n",
        "\n",
        "        question_patterns = [\n",
        "            r'\\?',\n",
        "            r'\\b(what|how|why|when|where|who|which|can|could|would|will|should|do|does|did|is|are)\\b.*',\n",
        "        ]\n",
        "\n",
        "        for sentence in sentences:\n",
        "            sentence = sentence.strip().lower()\n",
        "            if not sentence:\n",
        "                continue\n",
        "\n",
        "            # Quick check for question patterns\n",
        "            for pattern in question_patterns:\n",
        "                if re.search(pattern, sentence):\n",
        "                    if len(sentence) > 5:  # Avoid very short matches\n",
        "                        questions.append(sentence[:100])  # Truncate for storage\n",
        "                    break\n",
        "\n",
        "        return {\n",
        "            'count': len(questions),\n",
        "            'questions': questions[:5],  # First 5 examples\n",
        "            'question_rate': len(questions) / len(sentences) if sentences else 0\n",
        "        }"
      ],
      "metadata": {
        "id": "wjT5mBNI6Sjw"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def find_longest_pause(self, channel_data: Dict[str, np.ndarray], sample_rate: int) -> Dict:\n",
        "\n",
        "\n",
        "        try:\n",
        "            # Combine channels for overall silence detection\n",
        "            if 'agent' in channel_data and 'customer' in channel_data:\n",
        "                combined_audio = channel_data['agent'] + channel_data['customer']\n",
        "            else:\n",
        "                # Use first available channel\n",
        "                audio_arrays = [v for k, v in channel_data.items() if k != 'separation_method' and isinstance(v, np.ndarray)]\n",
        "                if audio_arrays:\n",
        "                    combined_audio = audio_arrays[0]\n",
        "                else:\n",
        "                    return {'duration': 0.0, 'start': 0.0, 'end': 0.0}\n",
        "\n",
        "            # Find silence periods\n",
        "            window_size = int(0.1 * sample_rate)  # 100ms windows\n",
        "            silence_threshold = 0.005  # Very low energy threshold\n",
        "            min_pause_length = int(2.0 * sample_rate)  # Minimum 2 second pause\n",
        "\n",
        "            silence_windows = []\n",
        "            for i in range(0, len(combined_audio) - window_size, window_size):\n",
        "                window = combined_audio[i:i + window_size]\n",
        "                rms = np.sqrt(np.mean(window**2))\n",
        "\n",
        "                if rms < silence_threshold:\n",
        "                    silence_windows.append(i)\n",
        "\n",
        "            if not silence_windows:\n",
        "                return {'duration': 0.0, 'start': 0.0, 'end': 0.0}\n",
        "\n",
        "            # Find longest consecutive silence\n",
        "            longest_pause = {'duration': 0.0, 'start': 0.0, 'end': 0.0}\n",
        "            current_start = None\n",
        "\n",
        "            for i, window_idx in enumerate(silence_windows):\n",
        "                if i == 0 or window_idx != silence_windows[i-1] + window_size:\n",
        "                    # Start of new silence period\n",
        "                    current_start = window_idx / sample_rate\n",
        "\n",
        "                if i == len(silence_windows) - 1 or window_idx + window_size != silence_windows[i+1]:\n",
        "                    # End of silence period\n",
        "                    current_end = (window_idx + window_size) / sample_rate\n",
        "                    current_duration = current_end - current_start\n",
        "\n",
        "                    if current_duration > longest_pause['duration']:\n",
        "                        longest_pause = {\n",
        "                            'duration': round(current_duration, 2),\n",
        "                            'start': round(current_start, 2),\n",
        "                            'end': round(current_end, 2)\n",
        "                        }\n",
        "\n",
        "            return longest_pause\n",
        "\n",
        "        except Exception as e:\n",
        "            self.logger.error(f\"Error finding longest pause: {e}\")\n",
        "            return {'duration': 0.0, 'start': 0.0, 'end': 0.0}\n",
        ""
      ],
      "metadata": {
        "id": "tKPqHrwI6fAz"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def analyze_sentiment_fast(self, text: str) -> Dict:\n",
        "\n",
        "\n",
        "        self.logger.info(\"Fast sentiment analysis...\")\n",
        "\n",
        "        if not self.sentiment_analyzer or not text.strip():\n",
        "            return {\n",
        "                'dominant_sentiment': 'neutral',\n",
        "                'confidence': 0.0,\n",
        "                'all_sentiments': {'neutral': 100.0}\n",
        "            }\n",
        "\n",
        "        try:\n",
        "            # Process in smaller chunks for speed\n",
        "            max_length = 300  # Shorter chunks for speed\n",
        "            chunks = [text[i:i+max_length] for i in range(0, len(text), max_length)]\n",
        "            chunks = [chunk.strip() for chunk in chunks if chunk.strip()]\n",
        "\n",
        "            if not chunks:\n",
        "                return {\n",
        "                    'dominant_sentiment': 'neutral',\n",
        "                    'confidence': 0.0,\n",
        "                    'all_sentiments': {'neutral': 100.0}\n",
        "                }\n",
        "\n",
        "            # Limit to first 3 chunks for speed in free Colab\n",
        "            chunks = chunks[:3]\n",
        "\n",
        "            # Analyze chunks\n",
        "            sentiment_scores = {'positive': 0, 'negative': 0, 'neutral': 0}\n",
        "            total_confidence = 0\n",
        "\n",
        "            for chunk in chunks:\n",
        "                try:\n",
        "                    result = self.sentiment_analyzer(chunk)\n",
        "\n",
        "                    if isinstance(result, list) and result:\n",
        "                        if isinstance(result[0], list):\n",
        "                            # Multiple scores format\n",
        "                            for score_dict in result[0]:\n",
        "                                label = score_dict['label'].lower()\n",
        "                                score = score_dict['score']\n",
        "\n",
        "                                if 'pos' in label or label == 'positive':\n",
        "                                    sentiment_scores['positive'] += score\n",
        "                                elif 'neg' in label or label == 'negative':\n",
        "                                    sentiment_scores['negative'] += score\n",
        "                                else:\n",
        "                                    sentiment_scores['neutral'] += score\n",
        "\n",
        "                                total_confidence += score\n",
        "                        else:\n",
        "                            # Single score format\n",
        "                            label = result[0]['label'].lower()\n",
        "                            score = result[0]['score']\n",
        "\n",
        "                            if 'pos' in label or label == 'positive':\n",
        "                                sentiment_scores['positive'] += score\n",
        "                            elif 'neg' in label or label == 'negative':\n",
        "                                sentiment_scores['negative'] += score\n",
        "                            else:\n",
        "                                sentiment_scores['neutral'] += score\n",
        "\n",
        "                            total_confidence += score\n",
        "\n",
        "                except Exception as e:\n",
        "                    self.logger.warning(f\"Error analyzing chunk sentiment: {e}\")\n",
        "                    continue\n",
        "\n",
        "            # Normalize scores\n",
        "            if total_confidence > 0:\n",
        "                sentiment_percentages = {\n",
        "                    k: round((v / total_confidence) * 100, 2)\n",
        "                    for k, v in sentiment_scores.items()\n",
        "                }\n",
        "            else:\n",
        "                sentiment_percentages = {'neutral': 100.0}\n",
        "\n",
        "            # Find dominant sentiment\n",
        "            dominant = max(sentiment_percentages.items(), key=lambda x: x[1])\n",
        "\n",
        "            return {\n",
        "                'dominant_sentiment': dominant[0],\n",
        "                'confidence': dominant[1],\n",
        "                'all_sentiments': sentiment_percentages\n",
        "            }\n",
        "\n",
        "        except Exception as e:\n",
        "            self.logger.error(f\"Error in fast sentiment analysis: {e}\")\n",
        "            return {\n",
        "                'dominant_sentiment': 'neutral',\n",
        "                'confidence': 0.0,\n",
        "                'all_sentiments': {'neutral': 100.0}\n",
        "            }"
      ],
      "metadata": {
        "id": "-p8TGWP46o6r"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_fast_insights(self, analysis_results: Dict) -> List[str]:\n",
        "\n",
        "\n",
        "        insights = []\n",
        "\n",
        "        try:\n",
        "            # Talk-time insights\n",
        "            talk_ratios = analysis_results.get('talk_time_ratio', {})\n",
        "            if 'agent' in talk_ratios and 'customer' in talk_ratios:\n",
        "                agent_ratio = talk_ratios['agent']\n",
        "                customer_ratio = talk_ratios['customer']\n",
        "\n",
        "                if agent_ratio > 70:\n",
        "                    insights.append(\"COACHING: Agent dominated conversation (>70%). Encourage more customer engagement.\")\n",
        "                elif agent_ratio < 30:\n",
        "                    insights.append(\"COACHING: Agent spoke too little (<30%). May need to provide more guidance.\")\n",
        "                elif 40 <= agent_ratio <= 60:\n",
        "                    insights.append(\"EXCELLENT: Well-balanced conversation ratio.\")\n",
        "\n",
        "                # Customer engagement\n",
        "                if customer_ratio > 60:\n",
        "                    insights.append(\"POSITIVE: High customer engagement - they're actively participating.\")\n",
        "                elif customer_ratio < 20:\n",
        "                    insights.append(\"ATTENTION: Low customer participation - may indicate disengagement.\")\n",
        "\n",
        "            # Question insights\n",
        "            questions = analysis_results.get('questions', {})\n",
        "            question_count = questions.get('count', 0)\n",
        "            call_duration = analysis_results.get('file_info', {}).get('duration_minutes', 1)\n",
        "\n",
        "            questions_per_minute = question_count / call_duration if call_duration > 0 else 0\n",
        "\n",
        "            if questions_per_minute < 0.5:\n",
        "                insights.append(\"COACHING: Very few questions asked. Train agent on discovery techniques.\")\n",
        "            elif questions_per_minute > 4:\n",
        "                insights.append(\"COACHING: Too many questions. Agent may not be listening to responses.\")\n",
        "            elif questions_per_minute >= 1:\n",
        "                insights.append(\"GOOD: Healthy questioning shows active engagement.\")\n",
        "\n",
        "            # Pause/silence insights\n",
        "            longest_pause = analysis_results.get('longest_pause', {})\n",
        "            pause_duration = longest_pause.get('duration', 0)\n",
        "\n",
        "            if pause_duration > 10:\n",
        "                insights.append(f\"ATTENTION: Very long pause ({pause_duration}s). Check for technical issues.\")\n",
        "            elif pause_duration > 5:\n",
        "                insights.append(f\"NOTE: Extended pause ({pause_duration}s) - may indicate thinking time or connection issues.\")\n",
        "            elif pause_duration < 1:\n",
        "                insights.append(\"FAST-PACED: Very few pauses - ensure customer has time to respond.\")\n",
        "\n",
        "            # Sentiment insights\n",
        "            sentiment = analysis_results.get('sentiment', {})\n",
        "            dominant_sentiment = sentiment.get('dominant_sentiment', 'neutral')\n",
        "            confidence = sentiment.get('confidence', 0)\n",
        "\n",
        "            if dominant_sentiment == 'negative' and confidence > 50:\n",
        "                insights.append(\"PRIORITY: Negative sentiment detected. Review for service recovery opportunities.\")\n",
        "            elif dominant_sentiment == 'positive' and confidence > 60:\n",
        "                insights.append(\"EXCELLENT: Strong positive sentiment - great customer experience!\")\n",
        "            elif dominant_sentiment == 'positive' and confidence > 40:\n",
        "                insights.append(\"GOOD: Positive customer sentiment maintained.\")\n",
        "\n",
        "            # Call duration insights\n",
        "            if call_duration < 1:\n",
        "                insights.append(\"QUICK CALL: Very short interaction - ensure resolution was complete.\")\n",
        "            elif call_duration > 10:\n",
        "                insights.append(\"LONG CALL: Extended duration - review for efficiency opportunities.\")\n",
        "\n",
        "            # Audio quality insights\n",
        "            separation_method = analysis_results.get('channel_separation', {}).get('separation_method', 'unknown')\n",
        "            if separation_method == 'mono_duplicate':\n",
        "                insights.append(\"AUDIO NOTE: Mono recording - speaker separation approximate.\")\n",
        "            elif separation_method == 'stereo_channels':\n",
        "                insights.append(\"AUDIO QUALITY: Clear stereo separation enables accurate speaker analysis.\")\n",
        "\n",
        "            # Default insight\n",
        "            if not insights:\n",
        "                insights.append(\"STANDARD CALL: No critical issues identified in this interaction.\")\n",
        "\n",
        "            # Limit insights for readability\n",
        "            return insights[:6]\n",
        "\n",
        "        except Exception as e:\n",
        "            self.logger.error(f\"Error generating insights: {e}\")\n",
        "            return [\"Unable to generate insights due to analysis error.\"]\n",
        ""
      ],
      "metadata": {
        "id": "Pw1r8NpK60Mz"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def analyze_call_optimized(self, audio_path: str, save_results: bool = True) -> Optional[Dict]:\n",
        "\n",
        "        self.logger.info(f\"FAST ANALYSIS: {Path(audio_path).name}\")\n",
        "        self.logger.info(\"=\" * 50)\n",
        "\n",
        "        start_time = datetime.now()\n",
        "\n",
        "        try:\n",
        "            # Validate input\n",
        "            if not Path(audio_path).exists():\n",
        "                self.logger.error(f\"Audio file not found: {audio_path}\")\n",
        "                return None\n",
        "\n",
        "            # Step 1: Fast preprocessing\n",
        "            audio_data, sample_rate, duration, is_stereo = self.preprocess_audio_fast(audio_path)\n",
        "            if audio_data is None:\n",
        "                return None\n",
        "\n",
        "            # Step 2: Channel separation (if stereo)\n",
        "            channel_data = {}\n",
        "            if self.enable_stereo_separation and is_stereo:\n",
        "                channel_data = self.separate_stereo_channels(audio_path)\n",
        "            else:\n",
        "                # Mono fallback\n",
        "                channel_data = {\n",
        "                    'agent': audio_data,\n",
        "                    'customer': audio_data,\n",
        "                    'separation_method': 'mono_assumed'\n",
        "                }\n",
        "\n",
        "            # Step 3: Fast transcription\n",
        "            transcription_result = self.transcribe_audio_fast(audio_path)\n",
        "            if not transcription_result:\n",
        "                self.logger.error(\"Transcription failed\")\n",
        "                return None\n",
        "\n",
        "            full_text = transcription_result['text']\n",
        "\n",
        "            # Step 4: Quick calculations\n",
        "            self.logger.info(\"Calculating metrics...\")\n",
        "\n",
        "            talk_time_ratio = self.calculate_talk_time_from_channels(channel_data, sample_rate)\n",
        "            questions = self.count_questions_fast(full_text)\n",
        "            longest_pause = self.find_longest_pause(channel_data, sample_rate)\n",
        "            sentiment = self.analyze_sentiment_fast(full_text)\n",
        "\n",
        "            # Compile results\n",
        "            results = {\n",
        "                'analysis_metadata': {\n",
        "                    'analyzer_version': '1.0.0-optimized',\n",
        "                    'analysis_timestamp': datetime.now().isoformat(),\n",
        "                    'processing_time_seconds': (datetime.now() - start_time).total_seconds(),\n",
        "                    'optimization_level': 'colab_free_tier',\n",
        "                    'models_used': {\n",
        "                        'whisper': self.whisper_model_size,\n",
        "                        'diarization': 'stereo_channel_separation',\n",
        "                        'sentiment': 'distilbert-base-uncased-finetuned-sst-2-english'\n",
        "                    }\n",
        "                },\n",
        "                'file_info': {\n",
        "                    'filename': Path(audio_path).name,\n",
        "                    'file_path': str(audio_path),\n",
        "                    'duration_seconds': round(duration, 2),\n",
        "                    'duration_minutes': round(duration / 60, 2),\n",
        "                    'sample_rate': sample_rate,\n",
        "                    'is_stereo': is_stereo,\n",
        "                    'file_size_mb': round(Path(audio_path).stat().st_size / (1024*1024), 2)\n",
        "                },\n",
        "                'talk_time_ratio': talk_time_ratio,\n",
        "                'questions': questions,\n",
        "                'longest_pause': longest_pause,\n",
        "                'sentiment': sentiment,\n",
        "                'transcription': {\n",
        "                    'full_text': full_text,\n",
        "                    'word_count': len(full_text.split()),\n",
        "                    'character_count': len(full_text)\n",
        "                },\n",
        "                'channel_separation': {\n",
        "                    'method': channel_data.get('separation_method', 'unknown'),\n",
        "                    'channels_available': len([k for k in channel_data.keys() if k != 'separation_method'])\n",
        "                }\n",
        "            }\n",
        "\n",
        "            # Generate insights\n",
        "            self.logger.info(\"Generating insights...\")\n",
        "            insights = self.generate_fast_insights(results)\n",
        "            results['actionable_insights'] = insights\n",
        "\n",
        "            # Save results if requested\n",
        "            if save_results:\n",
        "                self._save_results(results)\n",
        "\n",
        "            processing_time = (datetime.now() - start_time).total_seconds()\n",
        "            self.logger.info(f\"ANALYSIS COMPLETED in {processing_time:.2f} seconds\")\n",
        "            self.logger.info(\"=\" * 50)\n",
        "\n",
        "            return results\n",
        "\n",
        "        except Exception as e:\n",
        "            self.logger.error(f\"Error in optimized analysis: {e}\")\n",
        "            return None\n",
        ""
      ],
      "metadata": {
        "id": "QxRH_k497Ulm"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def _save_results(self, results: Dict):\n",
        "        \"\"\"Save analysis results to file\"\"\"\n",
        "        try:\n",
        "            # Create results directory\n",
        "            results_dir = PROJECT_ROOT / \"results\" / \"reports\"\n",
        "            results_dir.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "            # Generate filename\n",
        "            timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "            filename = f\"optimized_analysis_{timestamp}.json\"\n",
        "            filepath = results_dir / filename\n",
        "\n",
        "            # Save to JSON\n",
        "            with open(filepath, 'w', encoding='utf-8') as f:\n",
        "                json.dump(results, f, indent=2, ensure_ascii=False, default=str)\n",
        "\n",
        "            self.logger.info(f\"Results saved: {filepath}\")\n",
        "\n",
        "        except Exception as e:\n",
        "            self.logger.error(f\"Error saving results: {e}\")\n",
        ""
      ],
      "metadata": {
        "id": "HQS_WQUQ7pKa"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def display_results_compact(self, results: Dict):\n",
        "\n",
        "        if not results:\n",
        "            print(\"No results to display\")\n",
        "            return\n",
        "\n",
        "        print(\"\\n\" + \"=\"*60)\n",
        "        print(\"FAST CALL ANALYSIS REPORT\")\n",
        "        print(\"=\"*60)\n",
        "\n",
        "        # Processing info\n",
        "        metadata = results.get('analysis_metadata', {})\n",
        "        processing_time = metadata.get('processing_time_seconds', 0)\n",
        "        print(f\" Processed in: {processing_time:.1f} seconds\")\n",
        "\n",
        "        # File info\n",
        "        file_info = results['file_info']\n",
        "        print(f\" File: {file_info['filename']} ({file_info['duration_minutes']:.1f}m)\")\n",
        "\n",
        "        # Talk ratios\n",
        "        talk_ratios = results['talk_time_ratio']\n",
        "        print(f\"\\n TALK-TIME:\")\n",
        "        for speaker, ratio in talk_ratios.items():\n",
        "            emoji = \"👨‍💼\" if speaker == \"agent\" else \"👤\" if speaker == \"customer\" else \"🔊\"\n",
        "            print(f\" {emoji} {speaker.title()}: {ratio}%\")\n",
        "\n",
        "        # Questions\n",
        "        questions = results['questions']\n",
        "        print(f\"\\n Questions: {questions['count']} total\")\n",
        "\n",
        "        # Longest pause\n",
        "        pause = results['longest_pause']\n",
        "        if pause['duration'] > 1:\n",
        "            print(f\"Longest pause: {pause['duration']}s\")\n",
        "\n",
        "        # Sentiment\n",
        "        sentiment = results['sentiment']\n",
        "        sentiment_emoji = {\"positive\", \"negative\", \"neutral\"}\n",
        "        emoji = sentiment_emoji.get(sentiment['dominant_sentiment'])\n",
        "        print(f\"{emoji} Sentiment: {sentiment['dominant_sentiment'].title()} ({sentiment['confidence']:.1f}%)\")\n",
        "\n",
        "        # Key insights\n",
        "        insights = results.get('actionable_insights', [])\n",
        "        print(f\"\\nKEY INSIGHTS:\")\n",
        "        for i, insight in enumerate(insights[:3], 1):  # Show top 3\n",
        "            print(f\"   {i}. {insight}\")\n",
        "\n",
        "        if len(insights) > 3:\n",
        "            print(f\"   ... and {len(insights)-3} more insights\")\n",
        "\n",
        "        # Audio quality note\n",
        "        separation = results.get('channel_separation', {})\n",
        "        method = separation.get('method', 'unknown')\n",
        "        if method == 'stereo_channels':\n",
        "            print(f\"\\n Audio: Clear stereo separation\")\n",
        "        elif method == 'mono_assumed':\n",
        "            print(f\"\\n Audio: Mono (approximate speaker separation)\")\n",
        "\n",
        "        print(\"=\"*60)\n",
        "\n"
      ],
      "metadata": {
        "id": "5-HTHikh7vht"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def quick_analyze_call(audio_path: str, display_results: bool = True) -> Optional[Dict]:\n",
        "\n",
        "    analyzer = OptimizedCallQualityAnalyzer()\n",
        "    results = analyzer.analyze_call_optimized(audio_path)\n",
        "\n",
        "    if results and display_results:\n",
        "        analyzer.display_results_compact(results)\n",
        "\n",
        "    return results\n"
      ],
      "metadata": {
        "id": "I9kalzyW8UfU"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def analyze_demo_call(audio_path: str = None) -> Optional[Dict]:\n",
        "\n",
        "    if audio_path is None:\n",
        "        print(\"Please provide audio_path parameter\")\n",
        "        print(\"Usage: analyze_demo_call('path/to/your/audio.wav')\")\n",
        "        return None\n",
        "\n",
        "    if not Path(audio_path).exists():\n",
        "        print(f\"Audio file not found: {audio_path}\")\n",
        "        print(\"Please upload an audio file to Colab first\")\n",
        "        return None\n",
        "\n",
        "    print(\"Starting optimized call analysis...\")\n",
        "    print(f\"File: {Path(audio_path).name}\")\n",
        "    print(\"Optimized for free Colab (<30s processing)\")\n",
        "    print()\n",
        "\n",
        "    return quick_analyze_call(audio_path, display_results=True)\n"
      ],
      "metadata": {
        "id": "-pfpqJKt8crD"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class CoLabCallAnalyzer:\n",
        "\n",
        "    def __init__(self):\n",
        "        print(\"Initializing Colab Call Analyzer...\")\n",
        "        self.analyzer = OptimizedCallQualityAnalyzer(\n",
        "            whisper_model_size=\"tiny.en\",  # Fastest model\n",
        "            enable_stereo_separation=True,\n",
        "            log_level=\"WARNING\"  # Reduce output for cleaner Colab experience\n",
        "        )\n",
        "        print(\"Ready for analysis!\")\n",
        "\n",
        "    def analyze(self, audio_file_path: str) -> Optional[Dict]:\n",
        "\n",
        "        return self.analyzer.analyze_call_optimized(audio_file_path, save_results=False)\n",
        "\n",
        "    def show_results(self, results: Dict):\n",
        "\n",
        "        if results:\n",
        "            self.analyzer.display_results_compact(results)\n",
        "        else:\n",
        "            print(\"No results to display\")\n",
        "\n",
        "    def analyze_and_show(self, audio_file_path: str):\n",
        "\n",
        "        print(\"Analyzing call...\")\n",
        "        results = self.analyze(audio_file_path)\n",
        "        self.show_results(results)\n",
        "        return results\n",
        "\n"
      ],
      "metadata": {
        "id": "9HWaUwRp8muQ"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def benchmark_performance(audio_path: str, num_runs: int = 3) -> Dict:\n",
        "\n",
        "\n",
        "    print(f\"Benchmarking performance with {num_runs} runs...\")\n",
        "\n",
        "    times = []\n",
        "    analyzer = OptimizedCallQualityAnalyzer(log_level=\"ERROR\")\n",
        "\n",
        "    for i in range(num_runs):\n",
        "        print(f\"Run {i+1}/{num_runs}...\", end=\" \")\n",
        "        start_time = datetime.now()\n",
        "\n",
        "        results = analyzer.analyze_call_optimized(audio_path, save_results=False)\n",
        "\n",
        "        elapsed = (datetime.now() - start_time).total_seconds()\n",
        "        times.append(elapsed)\n",
        "\n",
        "        print(f\"{elapsed:.1f}s\")\n",
        "\n",
        "    stats = {\n",
        "        'average_time': round(np.mean(times), 2),\n",
        "        'min_time': round(min(times), 2),\n",
        "        'max_time': round(max(times), 2),\n",
        "        'std_deviation': round(np.std(times), 2),\n",
        "        'all_times': times,\n",
        "        'colab_compatible': max(times) < 30  # Under 30s requirement\n",
        "    }\n",
        "\n",
        "    print(f\"\\n PERFORMANCE RESULTS:\")\n",
        "    print(f\"   Average: {stats['average_time']}s\")\n",
        "    print(f\"   Range: {stats['min_time']}s - {stats['max_time']}s\")\n",
        "    print(f\"   Colab Compatible: {'YES' if stats['colab_compatible'] else 'NO'}\")\n",
        "\n",
        "    return stats\n"
      ],
      "metadata": {
        "id": "fsG-IqzD9CBB"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def setup_colab_environment():\n",
        "\n",
        "    print(\"Setting up Colab environment...\")\n",
        "\n",
        "    # Install required packages\n",
        "    install_commands = [\n",
        "        \"pip install -q whisper-openai\",\n",
        "        \"pip install -q librosa\",\n",
        "        \"pip install -q transformers\",\n",
        "        \"pip install -q torch torchaudio\",\n",
        "        \"pip install -q soundfile\",\n",
        "        \"pip install -q pydub\"\n",
        "    ]\n",
        "\n",
        "    for cmd in install_commands:\n",
        "        print(f\"Installing: {cmd.split()[-1]}\")\n",
        "        os.system(cmd)\n",
        "\n",
        "    print(\"Packages installed\")\n",
        "\n",
        "    # Pre-download models for faster first run\n",
        "    print(\"Pre-downloading models...\")\n",
        "    try:\n",
        "        # Download Whisper tiny.en\n",
        "        import whisper\n",
        "        whisper.load_model(\"tiny.en\")\n",
        "        print(\"Whisper tiny.en ready\")\n",
        "\n",
        "        # Download sentiment model\n",
        "        from transformers import pipeline\n",
        "        pipeline(\"sentiment-analysis\", model=\"distilbert-base-uncased-finetuned-sst-2-english\")\n",
        "        print(\"Sentiment model ready\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Model download warning: {e}\")\n",
        "\n",
        "    print(\"Colab environment ready for call analysis!\")\n"
      ],
      "metadata": {
        "id": "AS1emOvA9Rak"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_sample_colab_notebook():\n",
        "    \"\"\"\n",
        "    Generate sample code for Colab notebook\n",
        "    \"\"\"\n",
        "\n",
        "    notebook_code = '''\n",
        "# Call Quality Analyzer - Colab Demo\n",
        "# Optimized for sub-30 second processing\n",
        "\n",
        "# 1. Setup (run once)\n",
        "!pip install -q whisper-openai librosa transformers torch soundfile pydub\n",
        "\n",
        "# 2. Import and initialize\n",
        "from analyzer_optimized import CoLabCallAnalyzer\n",
        "analyzer = CoLabCallAnalyzer()\n",
        "\n",
        "# 3. Upload your audio file to Colab\n",
        "from google.colab import files\n",
        "uploaded = files.upload()\n",
        "audio_file = list(uploaded.keys())[0]\n",
        "\n",
        "# 4. Analyze the call\n",
        "results = analyzer.analyze_and_show(audio_file)\n",
        "\n",
        "# 5. Access specific results\n",
        "if results:\n",
        "    print(\"\\\\nDetailed breakdown:\")\n",
        "    print(f\"Agent talk time: {results['talk_time_ratio']['agent']}%\")\n",
        "    print(f\"Questions asked: {results['questions']['count']}\")\n",
        "    print(f\"Sentiment: {results['sentiment']['dominant_sentiment']}\")\n",
        "    '''\n",
        "\n",
        "    print(\" Sample Colab Notebook Code:\")\n",
        "    print(\"=\"*50)\n",
        "    print(notebook_code)\n",
        "    print(\"=\"*50)\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    print(\"Optimized Call Quality Analyzer\")\n",
        "    print(\"Designed for Google Colab free tier\")\n",
        "    print(\"Processes calls in <30 seconds\")\n",
        "    print()\n",
        "    print(\"Quick start options:\")\n",
        "    print(\"1. analyzer = CoLabCallAnalyzer()\")\n",
        "    print(\"2. results = analyzer.analyze_and_show('audio.wav')\")\n",
        "    print(\"3. Or use: quick_analyze_call('audio.wav')\")\n",
        "    print()\n",
        "    print(\"For Colab setup: setup_colab_environment()\")\n",
        "    print(\"For sample code: create_sample_colab_notebook()\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lITTZIsi9hRA",
        "outputId": "a5d3bc4a-a53d-495b-9c61-598d61238e3b"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Optimized Call Quality Analyzer\n",
            "Designed for Google Colab free tier\n",
            "Processes calls in <30 seconds\n",
            "\n",
            "Quick start options:\n",
            "1. analyzer = CoLabCallAnalyzer()\n",
            "2. results = analyzer.analyze_and_show('audio.wav')\n",
            "3. Or use: quick_analyze_call('audio.wav')\n",
            "\n",
            "For Colab setup: setup_colab_environment()\n",
            "For sample code: create_sample_colab_notebook()\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class StreamlinedAnalyzer:\n",
        "    \"\"\"\n",
        "    Minimal analyzer for basic metrics only\n",
        "    Even faster processing for simple use cases\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        self.whisper_model = whisper.load_model(\"tiny.en\")\n",
        "        if TRANSFORMERS_AVAILABLE:\n",
        "            self.sentiment_analyzer = pipeline(\"sentiment-analysis\")\n",
        "        else:\n",
        "            self.sentiment_analyzer = None\n",
        "\n",
        "    def basic_analysis(self, audio_path: str) -> Dict:\n",
        "        \"\"\"\n",
        "        Basic analysis with only essential metrics\n",
        "        Target: <15 seconds processing time\n",
        "        \"\"\"\n",
        "\n",
        "        start_time = datetime.now()\n",
        "\n",
        "        # Load audio (limit to 2 minutes max for speed)\n",
        "        audio, sr = librosa.load(audio_path, sr=16000, duration=120)\n",
        "        duration = len(audio) / sr\n",
        "\n",
        "        # Quick transcription\n",
        "        result = self.whisper_model.transcribe(audio_path, language=\"en\", verbose=False)\n",
        "        text = result['text']\n",
        "\n",
        "        # Basic sentiment (single chunk only)\n",
        "        sentiment = \"neutral\"\n",
        "        if self.sentiment_analyzer and text.strip():\n",
        "            try:\n",
        "                sent_result = self.sentiment_analyzer(text[:500])  # First 500 chars only\n",
        "                sentiment = sent_result[0]['label'].lower()\n",
        "            except:\n",
        "                pass\n",
        "\n",
        "        # Simple question count\n",
        "        question_count = text.count('?') + len([s for s in text.split() if s.lower() in ['what', 'how', 'why', 'when', 'where']])\n",
        "\n",
        "        processing_time = (datetime.now() - start_time).total_seconds()\n",
        "\n",
        "        return {\n",
        "            'duration_minutes': round(duration / 60, 2),\n",
        "            'word_count': len(text.split()),\n",
        "            'question_count': question_count,\n",
        "            'sentiment': sentiment,\n",
        "            'transcription': text[:200] + \"...\" if len(text) > 200 else text,\n",
        "            'processing_time': round(processing_time, 2)\n",
        "        }\n",
        "\n"
      ],
      "metadata": {
        "id": "VNdA2TKG98d9"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class BatchProcessor:\n",
        "    \"\"\"\n",
        "    Process multiple audio files efficiently in Colab\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        self.analyzer = OptimizedCallQualityAnalyzer(log_level=\"WARNING\")\n",
        "\n",
        "    def process_folder(self, folder_path: str, max_files: int = 5) -> List[Dict]:\n",
        "        \"\"\"\n",
        "        Process multiple audio files (limited for Colab performance)\n",
        "        \"\"\"\n",
        "\n",
        "        folder = Path(folder_path)\n",
        "        audio_extensions = {'.wav', '.mp3', '.m4a', '.flac'}\n",
        "        audio_files = [f for f in folder.glob('*') if f.suffix.lower() in audio_extensions]\n",
        "\n",
        "        # Limit files for Colab memory constraints\n",
        "        audio_files = audio_files[:max_files]\n",
        "\n",
        "        print(f\"Processing {len(audio_files)} files...\")\n",
        "\n",
        "        results = []\n",
        "        for i, file_path in enumerate(audio_files, 1):\n",
        "            print(f\"File {i}/{len(audio_files)}: {file_path.name}\")\n",
        "\n",
        "            try:\n",
        "                result = self.analyzer.analyze_call_optimized(str(file_path), save_results=False)\n",
        "                if result:\n",
        "                    results.append(result)\n",
        "                    print(f\"Completed in {result['analysis_metadata']['processing_time_seconds']:.1f}s\")\n",
        "                else:\n",
        "                    print(\"Failed\")\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"Error: {e}\")\n",
        "                continue\n",
        "\n",
        "        print(f\"\\n Batch complete: {len(results)}/{len(audio_files)} successful\")\n",
        "        return results\n",
        ""
      ],
      "metadata": {
        "id": "80lEEJdA-KxW"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_summary_report(self, results: List[Dict]) -> Dict:\n",
        "        \"\"\"\n",
        "        Create summary statistics from batch results\n",
        "        \"\"\"\n",
        "\n",
        "        if not results:\n",
        "            return {}\n",
        "\n",
        "        # Aggregate metrics\n",
        "        avg_duration = np.mean([r['file_info']['duration_minutes'] for r in results])\n",
        "        avg_questions = np.mean([r['questions']['count'] for r in results])\n",
        "\n",
        "        sentiment_counts = {}\n",
        "        for result in results:\n",
        "            sent = result['sentiment']['dominant_sentiment']\n",
        "            sentiment_counts[sent] = sentiment_counts.get(sent, 0) + 1\n",
        "\n",
        "        talk_ratios = []\n",
        "        for result in results:\n",
        "            if 'agent' in result['talk_time_ratio']:\n",
        "                talk_ratios.append(result['talk_time_ratio']['agent'])\n",
        "\n",
        "        avg_agent_talk = np.mean(talk_ratios) if talk_ratios else 50\n",
        "\n",
        "        summary = {\n",
        "            'total_calls': len(results),\n",
        "            'average_duration_minutes': round(avg_duration, 2),\n",
        "            'average_questions_per_call': round(avg_questions, 1),\n",
        "            'sentiment_distribution': sentiment_counts,\n",
        "            'average_agent_talk_percentage': round(avg_agent_talk, 1),\n",
        "            'processing_times': [r['analysis_metadata']['processing_time_seconds'] for r in results]\n",
        "        }\n",
        "\n",
        "        print(\" BATCH SUMMARY:\")\n",
        "        print(f\"   Total calls analyzed: {summary['total_calls']}\")\n",
        "        print(f\"   Average duration: {summary['average_duration_minutes']} minutes\")\n",
        "        print(f\"   Average questions: {summary['average_questions_per_call']}\")\n",
        "        print(f\"   Average agent talk: {summary['average_agent_talk_percentage']}%\")\n",
        "        print(f\"   Sentiment breakdown: {summary['sentiment_distribution']}\")\n",
        "\n",
        "        return summary"
      ],
      "metadata": {
        "id": "DmDn4J4t-XzX"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "RVmVGmGJ-fXu"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}